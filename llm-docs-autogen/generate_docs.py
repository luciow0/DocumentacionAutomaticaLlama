from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import os

# ğŸ‘‰ 1. Usar modelo Ollama (LLaMA 3)
llm = Ollama(model="llama3", request_timeout=120.0)

# ğŸ‘‰ 2. Configurar embeddings locales de HuggingFace
Settings.llm = llm  # tambiÃ©n aseguramos que el LLM se use globalmente
Settings.embed_model = HuggingFaceEmbedding(model_name="all-MiniLM-L6-v2")

# ğŸ‘‰ 3. Leer cÃ³digo fuente del proyecto
source_path = "src/main/java/com/martelli/meatmarket/"
docs = SimpleDirectoryReader(source_path).load_data()

# ğŸ‘‰ 4. Indexar el contenido con embeddings locales
index = VectorStoreIndex.from_documents(docs)

# ğŸ‘‰ 5. Motor de consulta a la IA
query_engine = index.as_query_engine()

# ğŸ‘‰ 6. Prompts para generar cada archivo de documentaciÃ³n
questions = {
    "docs/README.md": "GenerÃ¡ un README general para este proyecto backend con Spring Boot en castellano.",
    "docs/API_REFERENCE.md": "DocumentÃ¡ todos los endpoints definidos en este backend en castellano.",
    "docs/ARCHITECTURE.md": "ExplicÃ¡ la arquitectura general del sistema y cÃ³mo se conectan los mÃ³dulos en castellano.",
    "docs/DATABASE.md": "DescribÃ­ las entidades principales, sus campos y relaciones en castellano.",
}

# ğŸ‘‰ 7. Crear carpeta `docs` si no existe
os.makedirs("docs", exist_ok=True)

# ğŸ‘‰ 8. Ejecutar cada pregunta y guardar el resultado como Markdown
for file, prompt in questions.items():
    print(f"ğŸ“ Generando {file}...")
    response = query_engine.query(prompt)
    with open(file, "w", encoding="utf-8") as f:
        f.write(response.response)

print("âœ… Â¡DocumentaciÃ³n generada exitosamente!")
